{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"format1-WebScrape.ipynb","provenance":[{"file_id":"1WS_f8GUzp9DGSha-FNK5bkfw_yvPw5sR","timestamp":1585674502570}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1GAhmTAeT5wSjHjC5Uc9E2K8QwfkB8ers","authorship_tag":"ABX9TyNBcEE5bFUfefVOj12/kxAO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BMxDCn5f0NG8"},"source":["# Import libraries\n","import requests\n","import urllib.request\n","import time\n","from bs4 import BeautifulSoup\n","from google.colab import files\n","\n","\n","catalog_links = \"\"\n","\n","path = \"/content/drive/My Drive/US College Catalogs/MA/Scrape Links/\"\n","#------------------------------------\n","path_school = \"Fisher College/\"\n","#------------------------------------\n","path_file = \"list.txt\"\n","\n","with open(path + path_school + path_file, \"r\") as f:\n","  user_text = f.read()\n","#parse string into list\n","list_url_years = user_text.split()\n","for i in list_url_years:\n","  print(i)\n","\n","#----------------------------------------------\n","url_main = \"\"\n","#----------------------------------------------"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"06qFAJuo0cf8"},"source":["# First iteration:\n","# Gets links from first page year\n","url_get_text1 = []\n","\n","for page in list_url_years:\n","  response = requests.get(page)\n","  download_url = []\n","  soup = BeautifulSoup(response.text,'html.parser')\n","  #-------------------------------------------------\n","  result = soup.findAll(class_=\"msr-list\")\n","  #-------------------------------------------------\n","  soup = BeautifulSoup(str(result),'html.parser')\n","  for one_a_tag in soup.findAll('a'):\n","    try:\n","      link = one_a_tag['href']\n","      #print(url_main + link)\n","      download_url.append(url_main + link)\n","    except:\n","      print(\"link not found\")\n","  url_get_text1.append(download_url)\n","\n","for i in url_get_text1:\n","  for j in range(15):\n","    i.pop(0)\n","\n","for i in url_get_text1:\n","  print((i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xU-8a_T1LSwR","executionInfo":{"status":"ok","timestamp":1585754102182,"user_tz":240,"elapsed":2316,"user":{"displayName":"Yan Jie Hui","photoUrl":"","userId":"02400281036587505948"}},"outputId":"16d47f05-4b89-47b4-edac-4650db337abb","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["final = \"\"\n","try:\n","  response = requests.get(url_get_text1[0][0])\n","  soup = BeautifulSoup(response.text, 'html.parser')\n","  #----------------------------------------------\n","  result = soup.findAll(class_=\"maryann_arrow\")\n","  #----------------------------------------------\n","  soup = BeautifulSoup(str(result),'html.parser')\n","  for one_a_tag in soup.findAll('a'):\n","    try:\n","      link = one_a_tag['href']\n","      if link.find(\"./?prefix\") == 0:\n","        print(url_get_text1[0][0] + link)\n","        response = requests.get(url_get_text1[0][0] + link)\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        result = soup.findAll(class_=\"middle-area site-content-second\")\n","        soup = BeautifulSoup(str(result), 'html.parser')\n","        raw = soup.get_text()\n","        final = final + raw\n","    except:\n","      print(\"failed to access link : \",url_get_text1[0][0] + link)\n","except:\n","  print(\"failed to access link : \",url_get_text1[0][0])\n","print(final)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UCiCQzkT8T0r"},"source":["for i in url_get_text1:\n","  print(len(i)) \n","# each i corresponds to a different year\n","\n","n = 0 #year counter for output file\n","\n","for year in url_get_text1:\n","  final = \"\"\n","  for page in year:\n","    try:\n","      response = requests.get(page)\n","      download_url = []\n","      soup = BeautifulSoup(response.text, 'html.parser')\n","      result = soup.findAll(id=\"main\")\n","      soup = BeautifulSoup(str(result),'html.parser')\n","      for one_a_tag in soup.findAll('a'):  #'a' tags are for links\n","        try:\n","          link = one_a_tag['href']\n","          print(url_main + link)\n","\n","          response_out = requests.get(url_main + link)\n","          soup_out = BeautifulSoup(response_out.text, 'html.parser')\n","          course_desc = soup_out.findAll(id=\"main\")\n","          soup_out = BeautifulSoup(str(course_desc), 'html.parser')\n","          raw_text = soup_out.get_text()\n","          final = final + raw_text\n","        except:\n","          print(\"could not access link:\",url_main + link)\n","      #print(\"final =\",final)\n","    except:\n","      print(\"could not access link:\",page)\n","  with open(path + path_school + list_url_years[n].replace(\"/\",\"\") + \".txt\", 'a+') as f:\n","    f.write(final)\n","  n = n + 1"],"execution_count":null,"outputs":[]}]}