{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"simpleWebScrape.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1WS_f8GUzp9DGSha-FNK5bkfw_yvPw5sR","authorship_tag":"ABX9TyOc4Jh/OWDUO9UV0CXhZ38A"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"BMxDCn5f0NG8"},"source":["# Import libraries\n","import requests\n","import urllib.request\n","import time\n","from bs4 import BeautifulSoup\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s15sLSSP0N4I"},"source":["catalog_links = \"\"\n","path = \"/content/drive/My Drive/US College Catalogs/MA/Scrape Links/\"\n","###################################\n","path_school = \"Simmons University/\" ## only line to change\n","###################################\n","path_file = \"list.txt\"\n","\n","with open(path + path_school + path_file, \"r\") as f:\n","  user_text = f.read()\n","#parse string into list\n","list_url_years = user_text.split()\n","for i in list_url_years:\n","  print(i)\n","\n","url_main = \"http://simmons.smartcatalogiq.com\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"06qFAJuo0cf8"},"source":["url_get_text1 = []\n","\n","for page in list_url_years:\n","  response = requests.get(page)\n","  download_url = []\n","  soup = BeautifulSoup(response.text, 'html.parser')\n","  for one_a_tag in soup.findAll('a'):  #'a' tags are for links\n","    try:\n","#     if one_a_tag['class'] == \"msr-list\":\n","      link = one_a_tag['href']\n","      print(url_main + link)\n","      download_url.append(url_main + link)\n","    except:\n","      pass\n","  url_get_text1.append(download_url)\n","\n","for i in url_get_text1:\n","  print(len(i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UCiCQzkT8T0r"},"source":["for i in url_get_text1:\n","  print(len(i))\n","\n","for year in url_get_text1:\n","  for page in year:\n","    try:\n","      response = requests.get(page)\n","      download_url = []\n","      soup = BeautifulSoup(response.text, 'html.parser')\n","      for one_a_tag in soup.findAll('a'):  #'a' tags are for links\n","        try:\n","          #if one_a_tag['class'] == \"maryann_arrow\":\n","          link = one_a_tag['href']\n","          #print(url_main + link)\n","          download_url.append(url_main + link)\n","        except:\n","          pass\n","    except:\n","      print(\"could not access link:\",page)\n","  url_get_text1.append(download_url)\n","\n","for i in url_get_text1:\n","  print(len(i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNugyz742B_r"},"source":["n = 0;\n","for i in url_get_text1:\n","  final = \"\"\n","  for j in i:\n","    try:\n","      #print(\"accessing: \",j)\n","      response_out = requests.get(j)\n","      soup_out = BeautifulSoup(response_out.text, 'html.parser')\n","      raw_text = soup_out.get_text()\n","      final = final + raw_text\n","    except:\n","      print(\"could not access link:\",j)\n","  with open(path + path_school + list_url_years[n].replace(\"/\",\"\") + \".txt\", 'w') as f:\n","    f.write(final)\n","  n = n + 1"],"execution_count":null,"outputs":[]}]}